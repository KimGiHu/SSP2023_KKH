{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 88\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "# import necessary pacakages\n",
    "# strange issue: keep the import order to prevent matplotlib error\n",
    "#  import matplotlib -> librosa -> pyplot -> librosa.display\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "#from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "import os \n",
    "\n",
    "# Classification of voiced, unvoiced, and silent\n",
    "voiced = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'axr', 'ay', 'eh', 'er', 'ey', 'ih',\\\n",
    "        'ix', 'iy', 'ow', 'oy', 'uh', 'uw', 'ux', 'b', 'd', 'dh', 'el', 'em', \\\n",
    "        'en', 'g', 'jh', 'l', 'm', 'n', 'ng', 'nx', 'r', 'v', 'w', 'wh', 'y', \\\n",
    "        'z', 'zh', 'eng', 'hv', 'bcl', 'dcl', 'gcl']\n",
    "unvoiced = ['ch', 'dx', 'f', 'hh', 'k', 'p', 'q', 's', 'sh', 't', 'th', 'ax-h']\n",
    "silence = ['kcl', 'pcl', 'tcl', 'pau', 'epi', 'h#']\n",
    "\n",
    "### Dividing the dataset into train and validation sets\n",
    "# The TIMIT dataset is divided into 8 dialect regions, each with 50 speakers.\n",
    "# We will use 80% of the speakers for training and 20% for validation.\n",
    "# The train and validation sets are saved in the lists train and val respectively.\n",
    "# Each element of the list is the path to the speaker's folder.\n",
    " \n",
    "timit_path = './audio/timit_wav/'\n",
    "dialects = os.listdir(timit_path+'train')\n",
    "dialects = sorted(dialects)[1:]\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "\n",
    "for dialect in dialects:\n",
    "    speakers = os.listdir(timit_path+'train/'+dialect)\n",
    "    # if '.DS_Store' in speakers: speakers.remove('.DS_Store')\n",
    "    train_speakers = speakers[:int(len(speakers)*0.8)]\n",
    "    val_speakers = speakers[int(len(speakers)*0.8):]\n",
    "    for i in range(len(train_speakers)):\n",
    "        train.append(timit_path+'train/'+dialect+'/'+train_speakers[i])\n",
    "    for i in range(len(val_speakers)):\n",
    "        val.append(timit_path+'train/'+dialect+'/'+val_speakers[i])\n",
    "\n",
    "print(len(train), len(val))\n",
    "\n",
    "### Reading in the test set\n",
    "# The test set is stored in the list test.\n",
    "# Each element of the list is the path to the speaker's folder.\n",
    "# The test set is divided into 8 dialect regions, each with 24 speakers.\n",
    "\n",
    "test = []\n",
    "\n",
    "dialects_test = os.listdir(timit_path+'test')\n",
    "dialects_test = sorted(dialects_test)\n",
    "# if '.DS_Store' in dialects_test: dialects_test.remove('.DS_Store')\n",
    "\n",
    "for dialect in dialects_test:\n",
    "    speakers = os.listdir(timit_path+'test/'+dialect)\n",
    "    # if '.DS_Store' in speakers: speakers.remove('.DS_Store')\n",
    "    for i in range(len(speakers)):\n",
    "        test.append(timit_path+'test/'+dialect+'/'+speakers[i])\n",
    "\n",
    "print(len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./audio/timit_wav/train/dr2/fmmh0', './audio/timit_wav/train/dr2/mjde0', './audio/timit_wav/train/dr2/msat0', './audio/timit_wav/train/dr2/mprb0', './audio/timit_wav/train/dr2/fjkl0', './audio/timit_wav/train/dr2/mdps0', './audio/timit_wav/train/dr2/marc0', './audio/timit_wav/train/dr2/mrjt0', './audio/timit_wav/train/dr2/mkaj0', './audio/timit_wav/train/dr2/fsrh0', './audio/timit_wav/train/dr2/mjae0', './audio/timit_wav/train/dr2/mefg0', './audio/timit_wav/train/dr2/mdmt0', './audio/timit_wav/train/dr2/mtbc0', './audio/timit_wav/train/dr2/fdxw0', './audio/timit_wav/train/dr2/flmc0', './audio/timit_wav/train/dr2/mjhi0', './audio/timit_wav/train/dr2/mmds0', './audio/timit_wav/train/dr2/mrms0', './audio/timit_wav/train/dr2/mjbg0', './audio/timit_wav/train/dr2/fcaj0', './audio/timit_wav/train/dr2/mrjm0', './audio/timit_wav/train/dr2/mhrm0', './audio/timit_wav/train/dr2/fcyl0', './audio/timit_wav/train/dr2/mrjh0', './audio/timit_wav/train/dr2/mzmb0', './audio/timit_wav/train/dr2/mrcw0', './audio/timit_wav/train/dr2/mtat1', './audio/timit_wav/train/dr2/mrgs0', './audio/timit_wav/train/dr2/mtjg0', './audio/timit_wav/train/dr2/fhlm0', './audio/timit_wav/train/dr2/mppc0', './audio/timit_wav/train/dr2/mmxs0', './audio/timit_wav/train/dr2/fmjb0', './audio/timit_wav/train/dr2/mjrp0', './audio/timit_wav/train/dr2/fkaa0', './audio/timit_wav/train/dr2/mbjv0', './audio/timit_wav/train/dr2/feac0', './audio/timit_wav/train/dr2/fcmm0', './audio/timit_wav/train/dr2/fdas1', './audio/timit_wav/train/dr2/fdnc0', './audio/timit_wav/train/dr2/mwsb0', './audio/timit_wav/train/dr2/faem0', './audio/timit_wav/train/dr2/fpjf0', './audio/timit_wav/train/dr2/frll0', './audio/timit_wav/train/dr2/mrfk0', './audio/timit_wav/train/dr2/mdwd0', './audio/timit_wav/train/dr2/fscn0', './audio/timit_wav/train/dr2/mdss0', './audio/timit_wav/train/dr2/mrlr0', './audio/timit_wav/train/dr2/mdem0', './audio/timit_wav/train/dr2/mkjo0', './audio/timit_wav/train/dr2/mkdt0', './audio/timit_wav/train/dr2/mjmd0', './audio/timit_wav/train/dr2/fskl0', './audio/timit_wav/train/dr2/fajw0', './audio/timit_wav/train/dr2/mjeb0', './audio/timit_wav/train/dr2/ftmg0', './audio/timit_wav/train/dr2/mcew0', './audio/timit_wav/train/dr2/fmkf0', './audio/timit_wav/train/dr3/makr0', './audio/timit_wav/train/dr3/fcke0', './audio/timit_wav/train/dr3/mcdd0', './audio/timit_wav/train/dr3/fsls0', './audio/timit_wav/train/dr3/feme0', './audio/timit_wav/train/dr3/fdjh0', './audio/timit_wav/train/dr3/mcal0', './audio/timit_wav/train/dr3/mjjb0', './audio/timit_wav/train/dr3/mrds0', './audio/timit_wav/train/dr3/mwgr0', './audio/timit_wav/train/dr3/mhjb0', './audio/timit_wav/train/dr3/mjda0', './audio/timit_wav/train/dr3/mkxl0', './audio/timit_wav/train/dr3/mbef0', './audio/timit_wav/train/dr3/fntb0', './audio/timit_wav/train/dr3/fmjf0', './audio/timit_wav/train/dr3/mdss1', './audio/timit_wav/train/dr3/fdfb0', './audio/timit_wav/train/dr3/mdlh0', './audio/timit_wav/train/dr3/mdjm0', './audio/timit_wav/train/dr3/mtjm0', './audio/timit_wav/train/dr3/fpaz0', './audio/timit_wav/train/dr3/flac0', './audio/timit_wav/train/dr3/mree0', './audio/timit_wav/train/dr3/mgaf0', './audio/timit_wav/train/dr3/mrbc0', './audio/timit_wav/train/dr3/mkls1', './audio/timit_wav/train/dr3/fgrw0', './audio/timit_wav/train/dr3/mtpp0', './audio/timit_wav/train/dr3/mdhs0', './audio/timit_wav/train/dr3/mdef0', './audio/timit_wav/train/dr3/mdns0', './audio/timit_wav/train/dr3/mcef0', './audio/timit_wav/train/dr3/mddc0', './audio/timit_wav/train/dr3/mfmc0', './audio/timit_wav/train/dr3/madc0', './audio/timit_wav/train/dr3/milb0', './audio/timit_wav/train/dr3/mdwm0', './audio/timit_wav/train/dr3/mrjb1', './audio/timit_wav/train/dr3/mreh1', './audio/timit_wav/train/dr3/mwdk0', './audio/timit_wav/train/dr3/mtpg0', './audio/timit_wav/train/dr3/mprd0', './audio/timit_wav/train/dr3/mrwa0', './audio/timit_wav/train/dr3/fljd0', './audio/timit_wav/train/dr3/mapv0', './audio/timit_wav/train/dr3/mmeb0', './audio/timit_wav/train/dr3/fsjs0', './audio/timit_wav/train/dr3/mjlg1', './audio/timit_wav/train/dr3/mdtb0', './audio/timit_wav/train/dr3/mmam0', './audio/timit_wav/train/dr3/mjkr0', './audio/timit_wav/train/dr3/mlns0', './audio/timit_wav/train/dr3/mrtc0', './audio/timit_wav/train/dr3/mdbb1', './audio/timit_wav/train/dr3/mmsm0', './audio/timit_wav/train/dr3/fcmg0', './audio/timit_wav/train/dr3/mmjb1', './audio/timit_wav/train/dr3/fsjw0', './audio/timit_wav/train/dr3/fjlg0', './audio/timit_wav/train/dr4/mjee0', './audio/timit_wav/train/dr4/mrab1', './audio/timit_wav/train/dr4/msms0', './audio/timit_wav/train/dr4/maeb0', './audio/timit_wav/train/dr4/fbas0', './audio/timit_wav/train/dr4/mfrm0', './audio/timit_wav/train/dr4/fkdw0', './audio/timit_wav/train/dr4/mbwp0', './audio/timit_wav/train/dr4/msrg0', './audio/timit_wav/train/dr4/mtqc0', './audio/timit_wav/train/dr4/mjpm1', './audio/timit_wav/train/dr4/mjws0', './audio/timit_wav/train/dr4/mjls0', './audio/timit_wav/train/dr4/mstf0', './audio/timit_wav/train/dr4/mgxp0', './audio/timit_wav/train/dr4/mjsr0', './audio/timit_wav/train/dr4/mbma0', './audio/timit_wav/train/dr4/flhd0', './audio/timit_wav/train/dr4/falr0', './audio/timit_wav/train/dr4/msmc0', './audio/timit_wav/train/dr4/fpaf0', './audio/timit_wav/train/dr4/mljc0', './audio/timit_wav/train/dr4/fbmj0', './audio/timit_wav/train/dr4/mjmm0', './audio/timit_wav/train/dr4/mgjc0', './audio/timit_wav/train/dr4/mjdc0', './audio/timit_wav/train/dr4/mjlb0', './audio/timit_wav/train/dr4/mprt0', './audio/timit_wav/train/dr4/flkm0', './audio/timit_wav/train/dr4/mjac0', './audio/timit_wav/train/dr4/fcag0', './audio/timit_wav/train/dr4/mfwk0', './audio/timit_wav/train/dr4/mmgc0', './audio/timit_wav/train/dr4/mtrt0', './audio/timit_wav/train/dr4/mtas0', './audio/timit_wav/train/dr4/fsak0', './audio/timit_wav/train/dr4/mjrh0', './audio/timit_wav/train/dr4/mlsh0', './audio/timit_wav/train/dr4/mjxl0', './audio/timit_wav/train/dr4/marw0', './audio/timit_wav/train/dr4/fssb0', './audio/timit_wav/train/dr4/mpeb0', './audio/timit_wav/train/dr4/mrsp0', './audio/timit_wav/train/dr4/mljh0', './audio/timit_wav/train/dr4/mrgm0', './audio/timit_wav/train/dr4/mtrc0', './audio/timit_wav/train/dr4/mcss0', './audio/timit_wav/train/dr4/mgag0', './audio/timit_wav/train/dr4/mdma0', './audio/timit_wav/train/dr4/fklc0', './audio/timit_wav/train/dr4/mmdm0', './audio/timit_wav/train/dr4/mcdr0', './audio/timit_wav/train/dr4/mmbs0', './audio/timit_wav/train/dr4/fjwb1', './audio/timit_wav/train/dr5/mram0', './audio/timit_wav/train/dr5/mmdm1', './audio/timit_wav/train/dr5/mclm0', './audio/timit_wav/train/dr5/mjxa0', './audio/timit_wav/train/dr5/mdsj0', './audio/timit_wav/train/dr5/mmvp0', './audio/timit_wav/train/dr5/ftbw0', './audio/timit_wav/train/dr5/mjpg0', './audio/timit_wav/train/dr5/mdhl0', './audio/timit_wav/train/dr5/mtdp0', './audio/timit_wav/train/dr5/megj0', './audio/timit_wav/train/dr5/mges0', './audio/timit_wav/train/dr5/mgsh0', './audio/timit_wav/train/dr5/fsdc0', './audio/timit_wav/train/dr5/mrml0', './audio/timit_wav/train/dr5/fsjg0', './audio/timit_wav/train/dr5/ftlg0', './audio/timit_wav/train/dr5/mmwb0', './audio/timit_wav/train/dr5/fsag0', './audio/timit_wav/train/dr5/msas0', './audio/timit_wav/train/dr5/mmab1', './audio/timit_wav/train/dr5/mrld0', './audio/timit_wav/train/dr5/mfer0', './audio/timit_wav/train/dr5/mrew1', './audio/timit_wav/train/dr5/mjdm0', './audio/timit_wav/train/dr5/msrr0', './audio/timit_wav/train/dr5/fbjl0', './audio/timit_wav/train/dr5/mhmg0', './audio/timit_wav/train/dr5/mmcc0', './audio/timit_wav/train/dr5/mwem0', './audio/timit_wav/train/dr5/flmk0', './audio/timit_wav/train/dr5/fjxm0', './audio/timit_wav/train/dr5/mtat0', './audio/timit_wav/train/dr5/fcdr1', './audio/timit_wav/train/dr5/mwsh0', './audio/timit_wav/train/dr5/mrav0', './audio/timit_wav/train/dr5/flod0', './audio/timit_wav/train/dr5/mjrg0', './audio/timit_wav/train/dr5/mdas0', './audio/timit_wav/train/dr5/fsmm0', './audio/timit_wav/train/dr5/fbmh0', './audio/timit_wav/train/dr5/fdmy0', './audio/timit_wav/train/dr5/fpmy0', './audio/timit_wav/train/dr5/mbgt0', './audio/timit_wav/train/dr5/mtmt0', './audio/timit_wav/train/dr5/fexm0', './audio/timit_wav/train/dr5/mrvg0', './audio/timit_wav/train/dr5/fdtd0', './audio/timit_wav/train/dr5/fljg0', './audio/timit_wav/train/dr5/fear0', './audio/timit_wav/train/dr5/fsms1', './audio/timit_wav/train/dr5/mrkm0', './audio/timit_wav/train/dr5/mchl0', './audio/timit_wav/train/dr5/mhit0', './audio/timit_wav/train/dr5/mwac0', './audio/timit_wav/train/dr5/msem1', './audio/timit_wav/train/dr6/mpgr1', './audio/timit_wav/train/dr6/mcae0', './audio/timit_wav/train/dr6/fapb0', './audio/timit_wav/train/dr6/mabc0', './audio/timit_wav/train/dr6/fmju0', './audio/timit_wav/train/dr6/mrxb0', './audio/timit_wav/train/dr6/mrmb0', './audio/timit_wav/train/dr6/mkes0', './audio/timit_wav/train/dr6/fklc1', './audio/timit_wav/train/dr6/msds0', './audio/timit_wav/train/dr6/fbch0', './audio/timit_wav/train/dr6/mbma1', './audio/timit_wav/train/dr6/mejl0', './audio/timit_wav/train/dr6/mesj0', './audio/timit_wav/train/dr6/ftaj0', './audio/timit_wav/train/dr6/fhxs0', './audio/timit_wav/train/dr6/mtxs0', './audio/timit_wav/train/dr6/mdrd0', './audio/timit_wav/train/dr6/fsdj0', './audio/timit_wav/train/dr6/flag0', './audio/timit_wav/train/dr6/mjrk0', './audio/timit_wav/train/dr6/frjb0', './audio/timit_wav/train/dr6/msvs0', './audio/timit_wav/train/dr6/fpad0', './audio/timit_wav/train/dr6/mkln0', './audio/timit_wav/train/dr6/fsgf0', './audio/timit_wav/train/dr6/msat1', './audio/timit_wav/train/dr6/fsbk0', './audio/timit_wav/train/dr7/mtkd0', './audio/timit_wav/train/dr7/mjdg0', './audio/timit_wav/train/dr7/mjfr0', './audio/timit_wav/train/dr7/mhxl0', './audio/timit_wav/train/dr7/mjra0', './audio/timit_wav/train/dr7/mbom0', './audio/timit_wav/train/dr7/fjsk0', './audio/timit_wav/train/dr7/fleh0', './audio/timit_wav/train/dr7/msdb0', './audio/timit_wav/train/dr7/mses0', './audio/timit_wav/train/dr7/mtmn0', './audio/timit_wav/train/dr7/mfxs0', './audio/timit_wav/train/dr7/fcjs0', './audio/timit_wav/train/dr7/mfxv0', './audio/timit_wav/train/dr7/mkag0', './audio/timit_wav/train/dr7/maeo0', './audio/timit_wav/train/dr7/mvrw0', './audio/timit_wav/train/dr7/fkde0', './audio/timit_wav/train/dr7/madd0', './audio/timit_wav/train/dr7/fjhk0', './audio/timit_wav/train/dr7/mwre0', './audio/timit_wav/train/dr7/fblv0', './audio/timit_wav/train/dr7/mter0', './audio/timit_wav/train/dr7/mdpb0', './audio/timit_wav/train/dr7/mkdb0', './audio/timit_wav/train/dr7/mdcm0', './audio/timit_wav/train/dr7/mklr0', './audio/timit_wav/train/dr7/flet0', './audio/timit_wav/train/dr7/mtpr0', './audio/timit_wav/train/dr7/mdlr0', './audio/timit_wav/train/dr7/mpfu0', './audio/timit_wav/train/dr7/fpab1', './audio/timit_wav/train/dr7/mntw0', './audio/timit_wav/train/dr7/mmdg0', './audio/timit_wav/train/dr7/mdks0', './audio/timit_wav/train/dr7/mdlm0', './audio/timit_wav/train/dr7/mrem0', './audio/timit_wav/train/dr7/mtlc0', './audio/timit_wav/train/dr7/mcre0', './audio/timit_wav/train/dr7/mgak0', './audio/timit_wav/train/dr7/mjai0', './audio/timit_wav/train/dr7/mbar0', './audio/timit_wav/train/dr7/mgaw0', './audio/timit_wav/train/dr7/mtab0', './audio/timit_wav/train/dr7/mrpc1', './audio/timit_wav/train/dr7/mdlr1', './audio/timit_wav/train/dr7/mmws1', './audio/timit_wav/train/dr7/mafm0', './audio/timit_wav/train/dr7/mbbr0', './audio/timit_wav/train/dr7/fjen0', './audio/timit_wav/train/dr7/mtml0', './audio/timit_wav/train/dr7/msah1', './audio/timit_wav/train/dr7/mtwh1', './audio/timit_wav/train/dr7/mgar0', './audio/timit_wav/train/dr7/mhbs0', './audio/timit_wav/train/dr7/mpar0', './audio/timit_wav/train/dr7/freh0', './audio/timit_wav/train/dr7/fmkc0', './audio/timit_wav/train/dr7/mcth0', './audio/timit_wav/train/dr7/mwrp0', './audio/timit_wav/train/dr7/fpac0', './audio/timit_wav/train/dr8/mmpm0', './audio/timit_wav/train/dr8/mkrg0', './audio/timit_wav/train/dr8/mkdd0', './audio/timit_wav/train/dr8/fbcg1', './audio/timit_wav/train/dr8/mbcg0', './audio/timit_wav/train/dr8/fclt0', './audio/timit_wav/train/dr8/mmlm0', './audio/timit_wav/train/dr8/mcxm0', './audio/timit_wav/train/dr8/mmws0', './audio/timit_wav/train/dr8/fmbg0', './audio/timit_wav/train/dr8/mrdm0', './audio/timit_wav/train/dr8/fklh0', './audio/timit_wav/train/dr8/mmea0', './audio/timit_wav/train/dr8/fpls0', './audio/timit_wav/train/dr8/fceg0', './audio/timit_wav/train/dr8/fjrb0', './audio/timit_wav/train/dr8/mtcs0']\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_train_SA = [] # list형태로 파일을 받음\n",
    "speech_train_SX = [] # list형태로 파일을 받음\n",
    "speech_train_groundtruth_SA = []\n",
    "speech_train_groundtruth_SX = []\n",
    "# train 파일을 SA와 SX로 분류하여서 불러옴\n",
    "for i in range(len(train)):\n",
    "    train_files = os.listdir(train[i])\n",
    "    \n",
    "    for j in range(len(train_files)):\n",
    "        if train_files[j][-4:]==\".wav\":\n",
    "            \n",
    "            tmp, Fs = librosa.load(train[i]+\"/\"+train_files[j], sr=None, mono=True)\n",
    "            # 정답데이터를 받아오는 과정\n",
    "            answer = open(train[i]+\"/\"+train_files[j][:-4] + \".phn\", 'r')\n",
    "            train_answer = answer.readlines()\n",
    "            answer.close()\n",
    "            \n",
    "            # 정답 matrix 생성\n",
    "            # zeros : matrix size를 input으로 받아서 같은 사이즈의 zero-matrix를 생성\n",
    "            # zeros_like : matrix를 input으로 받아서 같은 사이즈의 zero-matrix를 생성\n",
    "            tmp2 = np.zeros_like(tmp)\n",
    "            # print(len(tmp), len(tmp2))\n",
    "\n",
    "\n",
    "            for k in range(len(train_answer)):\n",
    "                start, end, det = train_answer[k][:-1].split()\n",
    "                start = int(start)\n",
    "                end = int(end)\n",
    "                # print(start, end, det)\n",
    "\n",
    "                # silence : 0 / voiced : 1 / unvoiced : 2  << 라이브러리에 맞게끔 분류함.\n",
    "                if det in silence :\n",
    "                    tmp2[start:end] = 0\n",
    "                if det in voiced :\n",
    "                    tmp2[start:end] = 1\n",
    "                if det in unvoiced :\n",
    "                    tmp2[start:end] = 2\n",
    "\n",
    "            if train_files[j][:2]==\"sa\":\n",
    "                speech_train_SA.append(tmp)\n",
    "                speech_train_groundtruth_SA.append(tmp2)\n",
    "            if train_files[j][:2]==\"sx\":\n",
    "                speech_train_SX.append(tmp)\n",
    "                speech_train_groundtruth_SX.append(tmp2)\n",
    "\n",
    "speech_val_SA = [] # list형태로 파일을 받음\n",
    "speech_val_SX = [] # list형태로 파일을 받음\n",
    "speech_val_groundtruth_SA = []\n",
    "speech_val_groundtruth_SX = []\n",
    "tmp_array_val=[]\n",
    "# validation 파일을 SA와 SX로 분류하여서 불러옴\n",
    "for i in range(len(val)):\n",
    "    val_files = os.listdir(val[i])\n",
    "    \n",
    "    for j in range(len(val_files)):\n",
    "        if val_files[j][-4:]==\".wav\":\n",
    "            tmp, Fs = librosa.load(val[i]+\"/\"+val_files[j], sr=None, mono=True)\n",
    "            # 정답데이터를 받아오는 과정\n",
    "            answer = open(val[i]+\"/\"+val_files[j][:-4] + \".phn\", 'r')\n",
    "            val_answer = answer.readlines()\n",
    "            answer.close()\n",
    "            \n",
    "            # 정답 matrix 생성\n",
    "            # zeros : matrix size를 input으로 받아서 같은 사이즈의 zero-matrix를 생성\n",
    "            # zeros_like : matrix를 input으로 받아서 같은 사이즈의 zero-matrix를 생성\n",
    "            tmp2 = np.zeros_like(tmp)\n",
    "            # print(len(tmp), len(tmp2))\n",
    "\n",
    "\n",
    "            for k in range(len(val_answer)):\n",
    "                start, end, det = val_answer[k][:-1].split() # label에서 \\n을 제외하고 [:-1]로 설정함.\n",
    "                start = int(start) # start 값이 string형이기에 int형으로 casting해줌.\n",
    "                end = int(end) # end 값이 string형이기에 int형으로 casting해줌.\n",
    "                # print(start, end, det)\n",
    "\n",
    "                # silence : 0 / voiced : 1 / unvoiced : 2  << 라이브러리에 맞게끔 분류함.\n",
    "                if det in silence :\n",
    "                    tmp2[start:end] = 0\n",
    "                if det in voiced :\n",
    "                    tmp2[start:end] = 1\n",
    "                if det in unvoiced :\n",
    "                    tmp2[start:end] = 2\n",
    "\n",
    "            if val_files[j][:2]==\"sa\":\n",
    "                tmp_array_val.append(val[i]+\"/\"+val_files[j])\n",
    "                speech_val_SA.append(tmp)\n",
    "                speech_val_groundtruth_SA.append(tmp2)\n",
    "            if val_files[j][:2]==\"sx\":\n",
    "                speech_val_SX.append(tmp)\n",
    "                speech_val_groundtruth_SX.append(tmp2)\n",
    "    \n",
    "speech_test_SA = [] # list형태로 파일을 받음\n",
    "speech_test_SX = [] # list형태로 파일을 받음\n",
    "speech_test_groundtruth_SA = []\n",
    "speech_test_groundtruth_SX = []\n",
    "tmp_array_test=[]\n",
    "\n",
    "# test 파일을 SA와 SX로 분류하여서 불러옴\n",
    "for i in range(len(test)):\n",
    "    test_files = os.listdir(test[i])\n",
    "    \n",
    "    for j in range(len(test_files)):\n",
    "        if test_files[j][-4:]==\".wav\":\n",
    "            tmp, Fs = librosa.load(test[i]+\"/\"+test_files[j], sr=None, mono=True)\n",
    "            # 정답데이터를 받아오는 과정\n",
    "            answer = open(test[i]+\"/\"+test_files[j][:-4] + \".phn\", 'r')\n",
    "            test_answer = answer.readlines()\n",
    "            answer.close()\n",
    "            \n",
    "            # 정답 matrix 생성\n",
    "            # zeros : matrix size를 input으로 받아서 같은 사이즈의 zero-matrix를 생성\n",
    "            # zeros_like : matrix를 input으로 받아서 같은 사이즈의 zero-matrix를 생성\n",
    "            tmp2 = np.zeros_like(tmp)\n",
    "            # print(len(tmp), len(tmp2))\n",
    "\n",
    "\n",
    "            for k in range(len(test_answer)):\n",
    "                start, end, det = test_answer[k][:-1].split()\n",
    "                start = int(start)\n",
    "                end = int(end)\n",
    "                # print(start, end, det)\n",
    "\n",
    "                # silence : 0 / voiced : 1 / unvoiced : 2  << 라이브러리에 맞게끔 분류함.\n",
    "                if det in silence :\n",
    "                    tmp2[start:end] = 0\n",
    "                if det in voiced :\n",
    "                    tmp2[start:end] = 1\n",
    "                if det in unvoiced :\n",
    "                    tmp2[start:end] = 2\n",
    "            \n",
    "            if test_files[j][:2]==\"sa\":\n",
    "                tmp_array_test.append(test[i]+\"/\"+test_files[j])\n",
    "                speech_test_SA.append(tmp)\n",
    "                speech_test_groundtruth_SA.append(tmp2)\n",
    "            if test_files[j][:2]==\"sx\":\n",
    "                speech_test_SX.append(tmp)\n",
    "                speech_test_groundtruth_SX.append(tmp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./audio/timit_wav/test/dr1/mdab0/sa1.wav\n",
      "[-0.00720215 -0.00708008 -0.00668335 ... -0.009552   -0.00686646\n",
      " -0.00033569] [1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(tmp_array_test[0])\n",
    "print(speech_test_SA[0][39237: 40928], speech_test_groundtruth_SA[0][39237: 40928])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_dot(speech, Fs, Ts, Tf):\n",
    "    # sample_length = len(speechest_array[0,:])\n",
    "    Ns = int(Fs*Ts)\n",
    "    Nf = int(Fs*Tf)\n",
    "    # NFFT = int(2**(np.ceil(np.log2(Nf))))\n",
    "    # hNo = NFFT//2+1\n",
    "    Nt = len(speech[:])\n",
    "    nframes = int((Nt-Nf)//Ns + 1)\n",
    "\n",
    "    tmp_list_wiener = np.zeros(nframes)\n",
    "    v_n = np.zeros(nframes)\n",
    "\n",
    "    # for loop   \n",
    "    tmp_list_wiener = np.array([np.sqrt(np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf) for j in range(nframes)]) # list comression type\n",
    "\n",
    "    thresholding = (tmp_list_wiener[:10].mean() + tmp_list_wiener[10:].mean()) / 2\n",
    "        \n",
    "    v_n[:] = tmp_list_wiener > thresholding\n",
    "    thresholding_2nd = (tmp_list_wiener[v_n[:]==0].mean() + tmp_list_wiener[v_n[:]==1].mean()) / 2\n",
    "\n",
    "    v_n[:] = tmp_list_wiener > thresholding_2nd\n",
    "        \n",
    "    thresholding_3rd = (tmp_list_wiener[v_n[:]==0].mean() + tmp_list_wiener[v_n[:]==1].mean()) / 2\n",
    "    v_n[:] = tmp_list_wiener > thresholding_3rd\n",
    "    print(thresholding , thresholding_2nd, thresholding_3rd)\n",
    "    \n",
    "                \n",
    "    return v_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = 0.01\n",
    "Tf = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005503250941083789 0.009534124825075171 0.01202389125169841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_dot(speech_train_SA[0], Fs, Ts, Tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_UV_auto_correlation(speech, Fs, Ts, Tf):\n",
    "    # sample_length = len(speechest_array[0,:])\n",
    "    Ns = int(Fs*Ts)\n",
    "    Nf = int(Fs*Tf)\n",
    "    # NFFT = int(2**(np.ceil(np.log2(Nf))))\n",
    "    # hNo = NFFT//2+1\n",
    "    Nt = len(speech[:])\n",
    "    nframes = int((Nt-Nf)//Ns + 1)\n",
    "\n",
    "    tmp_list_wiener = np.zeros(nframes)\n",
    "    a = np.zeros(2)\n",
    "    v_n = np.zeros(nframes)\n",
    "\n",
    "    # for loop : 한 프레임마다 에너지를 계산함.  \n",
    "    tmp_list_wiener = np.array([np.sqrt(np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf) \\\n",
    "                                for j in range(nframes)]) # list comression type\n",
    "\n",
    "    thresholding = (tmp_list_wiener[:10].mean() + tmp_list_wiener[10:].mean()) / 2\n",
    "        \n",
    "    v_n[:] = tmp_list_wiener > thresholding\n",
    "    thresholding_2nd = (tmp_list_wiener[v_n[:]==0].mean() + tmp_list_wiener[v_n[:]==1].mean()) / 2\n",
    "\n",
    "    v_n[:] = tmp_list_wiener > thresholding_2nd\n",
    "        \n",
    "    thresholding_3rd = (tmp_list_wiener[v_n[:]==0].mean() + tmp_list_wiener[v_n[:]==1].mean()) / 2\n",
    "    v_n[:] = tmp_list_wiener > thresholding_3rd\n",
    "    \n",
    "    threshold_ac = 0.5\n",
    "    tmp = np.zeros(nframes)\n",
    "    for j in range(nframes):\n",
    "        if v_n[j] == 1:\n",
    "            a[0] = np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf\n",
    "            # print(a[0]) \n",
    "            a[1] = np.sum((speech[j*Ns:j*Ns+Nf])*(speech[j*Ns + 1:j*Ns+Nf + 1]))/Nf\n",
    "            \n",
    "            # print(a[1])\n",
    "            tmp[j] = a[1]/a[0]\n",
    "            # print(a[1]/a[0])\n",
    "            if a[1]/a[0] > threshold_ac :\n",
    "                v_n[j] = 1\n",
    "                \n",
    "            if a[1]/a[0] < threshold_ac :\n",
    "                v_n[j] = 2\n",
    "\n",
    "    threshold_ac_by_mean = tmp.mean()\n",
    "    for j in range(nframes):\n",
    "        if v_n[j] == 1:\n",
    "            a[0] = np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf \n",
    "            a[1] = np.sum((speech[j*Ns + 1:j*Ns+Nf])**2)/Nf\n",
    "            \n",
    "            if a[1]/a[0] > threshold_ac_by_mean :\n",
    "                v_n[j] = 1\n",
    "            if a[1]/a[0] < threshold_ac_by_mean :\n",
    "                v_n[j] = 2\n",
    "            \n",
    "    # to determine vocied and unvoiced speech\n",
    "    return v_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tmp_array_val))\n",
    "# check = 170\n",
    "# print(tmp_array_val[check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_uv = V_UV_auto_correlation(speech_val_SA[100], Fs, Ts, Tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "6\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "# print(v_uv[:]==1) # voiced 신호만 보기 위한 True/False Boolean 값\n",
    "print(len(v_uv[v_uv[:]==1])) # voiced 신호 갯수\n",
    "# print(v_uv[:]==2) # unvoiced 신호만 보기 위한 True/False Boolean 값\n",
    "print(len(v_uv[v_uv[:]==2])) # unvoiced 신호 갯수\n",
    "# print(v_uv[:]==0) # silence 신호만 보기 위한 True/False Boolean 값\n",
    "print(len(v_uv[v_uv[:]==0])) # silence 신호 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_UV_auto_correlation2(speech, Fs, Ts, Tf):\n",
    "    # sample_length = len(speechest_array[0,:])\n",
    "    Ns = int(Fs*Ts)\n",
    "    Nf = int(Fs*Tf)\n",
    "    # NFFT = int(2**(np.ceil(np.log2(Nf))))\n",
    "    # hNo = NFFT//2+1\n",
    "    Nt = len(speech[:])\n",
    "    nframes = int((Nt-Nf)//Ns + 1)\n",
    "\n",
    "    tmp_list_wiener = np.zeros(nframes)\n",
    "    a = np.zeros(2)\n",
    "    v_n = np.zeros(nframes)\n",
    "\n",
    "    # for loop : 한 프레임마다 에너지를 계산함.  \n",
    "    tmp_list_wiener = np.array([np.sqrt(np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf) \\\n",
    "                                for j in range(nframes)]) # list comression type\n",
    "\n",
    "    thresholding = (tmp_list_wiener[:10].mean() + tmp_list_wiener[10:].mean()) / 2\n",
    "        \n",
    "    v_n[:] = tmp_list_wiener > thresholding\n",
    "    thresholding_2nd = (tmp_list_wiener[v_n[:]==0].mean() + tmp_list_wiener[v_n[:]==1].mean()) / 2\n",
    "\n",
    "    v_n[:] = tmp_list_wiener > thresholding_2nd\n",
    "        \n",
    "    thresholding_3rd = (tmp_list_wiener[v_n[:]==0].mean() + tmp_list_wiener[v_n[:]==1].mean()) / 2\n",
    "    v_n[:] = tmp_list_wiener > thresholding_3rd\n",
    "    \n",
    "    threshold_ac = 0.5\n",
    "    tmp = np.zeros(nframes)\n",
    "    for j in range(nframes):\n",
    "        if v_n[j] == 1:\n",
    "            a[0] = np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf\n",
    "            # print(a[0]) \n",
    "            a[1] = np.sum((speech[j*Ns:j*Ns+Nf])*(speech[j*Ns + 1:j*Ns+Nf + 1]))/Nf\n",
    "            \n",
    "            # print(a[1])\n",
    "            tmp[j] = a[1]/a[0]\n",
    "            # print(a[1]/a[0])\n",
    "            if a[1]/a[0] > threshold_ac :\n",
    "                v_n[j] = 1\n",
    "                \n",
    "            if a[1]/a[0] < threshold_ac :\n",
    "                v_n[j] = 2\n",
    "\n",
    "    threshold_ac_by_mean = tmp.mean()\n",
    "    for j in range(nframes):\n",
    "        if v_n[j] == 1:\n",
    "            a[0] = np.sum((speech[j*Ns:j*Ns+Nf])**2)/Nf \n",
    "            a[1] = np.sum((speech[j*Ns + 1:j*Ns+Nf])**2)/Nf\n",
    "            \n",
    "            if a[1]/a[0] > threshold_ac_by_mean :\n",
    "                v_n[j] = 1\n",
    "            if a[1]/a[0] < threshold_ac_by_mean :\n",
    "                v_n[j] = 2\n",
    "    \n",
    "    v_uv = np.zeros_like(speech)\n",
    "    for k in range(nframes):\n",
    "            v_uv[k*Ns + 1:k*Ns+Nf] = v_n[k]\n",
    "            \n",
    "    # to determine vocied and unvoiced speech\n",
    "    return v_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple validation files autocorrelation\n",
    "v_uv_train_SA = []\n",
    "for i in range(len(speech_train_SA)):\n",
    "    v_uv2 = V_UV_auto_correlation2(speech_train_SA[i], Fs, Ts, Tf)\n",
    "    v_uv_train_SA.append(v_uv2)\n",
    "\n",
    "v_uv_train_SX = []\n",
    "for i in range(len(speech_val_SX)):\n",
    "    v_uv2 = V_UV_auto_correlation2(speech_train_SX[i], Fs, Ts, Tf)\n",
    "    v_uv_train_SX.append(v_uv2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple validation files autocorrelation\n",
    "v_uv_val_SA = []\n",
    "for i in range(len(speech_val_SA)):\n",
    "    v_uv3 = V_UV_auto_correlation2(speech_val_SA[i], Fs, Ts, Tf)\n",
    "    v_uv_val_SA.append(v_uv3)\n",
    "\n",
    "v_uv_val_SX = []\n",
    "for i in range(len(speech_val_SX)):\n",
    "    v_uv3 = V_UV_auto_correlation2(speech_val_SX[i], Fs, Ts, Tf)\n",
    "    v_uv_val_SX.append(v_uv3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(speech_test_groundtruth_SA[0][24000:24100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(1, 1, 51815)\n",
      "(1, 1, 51815)\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579301.8750 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 578936.5000 - accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 578386.0000 - accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 577728.3750 - accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 576999.2500 - accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 576224.8750 - accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 575429.2500 - accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 574635.7500 - accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 573865.6875 - accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 573138.3750 - accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 572469.2500 - accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 571869.5000 - accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 571345.8750 - accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 570900.2500 - accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 570531.0000 - accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 570233.0000 - accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 569999.0625 - accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 569820.7500 - accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 569689.5000 - accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 569596.8750 - accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 569535.3750 - accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 569498.2500 - accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 569480.0625 - accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 569476.4375 - accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 569483.8125 - accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 569499.5000 - accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 569521.6250 - accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 569548.5000 - accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 569579.3750 - accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 569613.2500 - accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 569649.7500 - accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 569688.6250 - accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 569729.3750 - accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 569772.1250 - accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 569816.7500 - accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 569863.1250 - accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 569911.2500 - accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 569961.1250 - accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 570012.8750 - accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 570066.3750 - accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 570121.6250 - accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 570178.7500 - accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 570237.7500 - accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 570298.5000 - accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 570361.2500 - accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 570425.7500 - accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 570492.1250 - accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 570560.5000 - accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 570630.7500 - accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 570703.0625 - accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 570777.1250 - accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 570853.3750 - accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 570931.3750 - accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 571011.5625 - accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 571093.6250 - accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 571177.6875 - accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 571263.8750 - accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 571352.0000 - accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 571442.1875 - accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 571534.4375 - accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 571628.7500 - accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 571725.1250 - accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 571823.6250 - accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 571924.1250 - accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 572026.7500 - accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 572131.5000 - accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 572238.3125 - accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 572347.2500 - accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 572458.3750 - accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 572571.5000 - accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 572686.8125 - accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 572804.1875 - accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 572923.9375 - accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 573045.5625 - accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 573169.5625 - accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 573295.5625 - accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 573423.7500 - accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 573554.1250 - accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 573686.6250 - accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 573821.3750 - accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 573958.1875 - accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 574097.3750 - accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 574238.5000 - accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 574382.0000 - accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 574527.5625 - accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 574675.3750 - accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 574825.3750 - accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 574977.6250 - accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 575132.0000 - accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 575288.5625 - accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 575447.3750 - accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 575608.3750 - accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 575771.5000 - accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 575937.0000 - accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 576104.5000 - accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 576274.5000 - accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 576446.6250 - accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 576620.6250 - accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 576797.3125 - accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 576975.8750 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a9e9c33a0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RNN, Dropout, Activation, Embedding, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# https://stackoverflow.com/questions/54416322/expected-ndim-3-found-ndim-2\n",
    "x_train = np.array(v_uv_train_SA[0], dtype=float)\n",
    "# print(x_train)\n",
    "# x_train = x_train.reshape(-1, 1, len(v_uv_train_SA[0]))\n",
    "\n",
    "y_train = np.array(speech_train_groundtruth_SA[0],dtype=float)\n",
    "# y_train = y_train.reshape(-1, 1, len(speech_train_groundtruth_SA[0]))\n",
    "# y_train = to_categorical(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_val, y_val = np.array(v_uv_val_SA[0],dtype=float), np.array(speech_val_SA[0],dtype=float)\n",
    "x_val = x_val.reshape(-1,1,len(x_val))\n",
    "y_val = y_val.reshape(-1,1,len(y_val))\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# definite the LSTM models\n",
    "model = Sequential()\n",
    "model.add(LSTM(250, input_shape=(1, len(v_uv_train_SA[0])), return_sequences=True))\n",
    "# model.add(LSTM(250, return_sequences=True))\n",
    "# model.add(LSTM(250))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(len(v_uv_train_SA[0]), input_shape=(1, len(v_uv_train_SA[0])), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# model ift\n",
    "# model.fit(v_uv_train_SA[0], speech_train_groundtruth_SA[0], epochs=100, batch_size=100, validation_data=(v_uv_val_SA[0], speech_val_SA[0]))\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=100,validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "# for i in range(len(v_uv_train_SA)):\n",
    "#     model.fit(v_uv_train_SA, speech_train_groundtruth_SA, epochs=100, batch_size=100)\n",
    "\n",
    "# for i in range(len(v_uv_val_SA)):\n",
    "#     model.fit(validation_data=(v_uv_val_SA, speech_val_SA))\n",
    "# model2.fit(x_train, y_train, epochs=100, batch_size=100, validation_data=(v_uv_val_SA, speech_val_SA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN by pytorch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SpeechRNN(nn.module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "#         super(SpeechRNN, self).__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.rnn = nn.RNN(input_size,hidden_size)\n",
    "#         self.layer = nn.Linear(hidden_size, self.output_size)\n",
    "    \n",
    "#     def forward(self, speech, prints=False):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
